{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8834893,"sourceType":"datasetVersion","datasetId":5316491}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dependencies which require restarting \n!pip install trl\n%pip install -U datasets\n!pip install unbabel-comet\n!pip install pyarrow==11.0.0\n\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env TOKENIZERS_PARALLELISM=false","metadata":{"execution":{"iopub.status.busy":"2024-08-12T19:07:20.957308Z","iopub.execute_input":"2024-08-12T19:07:20.957813Z","iopub.status.idle":"2024-08-12T19:07:20.993161Z","shell.execute_reply.started":"2024-08-12T19:07:20.957761Z","shell.execute_reply":"2024-08-12T19:07:20.991778Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"env: TOKENIZERS_PARALLELISM=false\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importing dependencies\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom transformers import NllbTokenizerFast, TrainingArguments, AutoModelForSeq2SeqLM\nfrom trl import PPOTrainer, PPOConfig, create_reference_model\nfrom datasets import Dataset\nimport warnings\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings(\"ignore\")\nfrom collections import deque\nfrom datasets import load_dataset\nfrom trl import IterativeSFTTrainer\nfrom comet import download_model, load_from_checkpoint\nimport statistics\nimport torch.nn.functional as F\nimport logging\nloggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\nfor logger in loggers:\n    logger.setLevel(logging.WARNING)\n\n# Setting device to GPU\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n#Loading reward generation model\nmodel_path = download_model(\"Unbabel/wmt22-cometkiwi-da\")\nreward_gen_model = load_from_checkpoint(model_path)\n\n\ngerman_pronouns = ['Er','Es','Sie','er','es','sie']","metadata":{"execution":{"iopub.status.busy":"2024-08-12T19:07:22.528916Z","iopub.execute_input":"2024-08-12T19:07:22.529263Z","iopub.status.idle":"2024-08-12T19:08:02.671790Z","shell.execute_reply.started":"2024-08-12T19:07:22.529233Z","shell.execute_reply":"2024-08-12T19:08:02.670990Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-08-12 19:07:30.598769: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-12 19:07:30.598869: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-12 19:07:30.734784: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5196f84e83694b0688d50c178efbec99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.ckpt:   0%|          | 0.00/2.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7126b0f9bf34893bb50a3365ac1c6aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c72d26947612440db796851f3690588f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44389129caf24ab2a050fc6e766ad12c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"LICENSE:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f3ba40451b6407d89799f045e103bab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"hparams.yaml:   0%|          | 0.00/710 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31830c394c2d4aab9b7b079ab2839d70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14973e2d02754befb78e8e9504fea0a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ea657e987b24def978fb4025e0c5be7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/513 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28aa9d658e6f46d5920afcafc7f96b92"}},"metadata":{}}]},{"cell_type":"code","source":"# Reward functions\ndef gen_hf(data,token_length,pro_data):\n    data_comet_inp = []\n    for i in range(len(data)):\n        data_comet = {}\n        data_comet[\"mt\"]=data[i][\"mt\"]\n        data_comet[\"src\"]=data[i][\"src\"]\n        #data_comet[\"ref\"]=data[i][\"ref\"]\n        data_comet_inp.append(data_comet)\n    output = reward_gen_model.predict(data_comet_inp,batch_size = len(data), gpus = 1,progress_bar=False)\n    rewards = output.scores\n    fin = []\n    fin_alt = []\n    '''\n    for i in range(len(data)):\n        print(f\"For candidate translation {i}: \")\n        print(\"Translation is: \", data[i][\"mt\"])\n        print(\"Comet rewards: \",rewards[i])\n        print(\"Pronoun rewards: \",pro_data[i])\n        print(\"Final reward: \",rewards[i] + (pro_data[i]/token_length))\n        fin.append(rewards[i] + (pro_data[i]/token_length))\n        print(\"Final reward(alt): \",rewards[i] + (pro_data[i]/10))\n        fin_alt.append(rewards[i] + (pro_data[i]/10))\n        print(\"\\n\")\n    '''\n    #print(\"Comet rewards: \",rewards)\n    pro_rew = pro_data\n    #print(\"Pronoun rewards: \",pro_rew)\n    final_reward = [0]*len(rewards)\n    for i in range(len(final_reward)):\n        final_reward[i] = rewards[i] + (pro_rew[i]/token_length)\n        #final_reward[i] = rewards[i] \n    # /20 \n    # /(No of tokens in the sentence)\n    # Discard sentences with length <= 5 in training set \n    #print(\"\\n\")\n    #print(\"Final rewards: \",final_reward)    \n    high_ind = np.argmax(final_reward)\n    y_high = data[high_ind][\"mt\"]\n    return y_high, high_ind \n\ndef calc_comet_score(data):\n    output = reward_gen_model.predict(data,batch_size = len(data), gpus = 1,progress_bar=False)\n    rewards = output.scores\n    #print(\"COMET reward: \",rewards[0])\n    return rewards[0]\n\ndef calc_val_comet_score(data):\n    output = reward_gen_model.predict(data,batch_size = len(data), gpus = 1,progress_bar=False)\n    rewards = output.scores\n    return sum(rewards)/len(rewards)\n\ndef get_pronoun_reward(response,k,cts):\n    pro_reward = 0\n    german_pros = ['Er','Es','Sie','er','es','sie']\n    fl = False\n    for i in range(len(response.sequences[0])):\n        token = tokenizer.decode(response.sequences[0][i])\n        for i in range(int(len(german_pros)/2)):\n            if (token == german_pros[i] or token == german_pros[i+3]):\n                    probabilities_tensor = torch.nn.functional.softmax(response.scores[i-1], dim=-1)\n                    max_prob_index = torch.argmax(probabilities_tensor)\n                    max_prob_value = probabilities_tensor.flatten()[max_prob_index]\n\n                    fl = True\n                    if tgt_pronouns[k] == german_pros[i]:\n                        pro_reward = max_prob_value.item()\n                        cts[2]+=1\n                    elif tgt_pronouns[k] == german_pros[i+3]:\n                        pro_reward = max_prob_value.item()\n                        cts[2]+=1\n                    else:\n                        pro_reward = -max_prob_value.item()\n                        cts[0]+=1\n    \n    if fl==False:\n        cts[1]+=1\n    return pro_reward,cts\n\n\ndef calc_pronoun_score(responses):\n    pro_reward = [0]*(len(responses))\n    k = -84\n    for i in range(len(responses)):\n        for j in range(len(responses[i].sequences[0])):\n            token = tokenizer.decode(responses[i].sequences[0][j])\n            if token in german_pronouns:\n                probabilities_tensor = torch.nn.functional.softmax(responses[i].scores[j-1], dim=-1)\n                max_prob_index = torch.argmax(probabilities_tensor)\n                max_prob_value = probabilities_tensor.flatten()[max_prob_index]\n                if token == tgt_pronouns[k]:\n                    pro_reward[i] = max_prob_value.item()\n                else:\n                    pro_reward[i] = -max_prob_value.item()\n        k = k + 1\n    return pro_reward","metadata":{"execution":{"iopub.status.busy":"2024-08-12T19:35:21.432974Z","iopub.execute_input":"2024-08-12T19:35:21.433728Z","iopub.status.idle":"2024-08-12T19:35:21.452459Z","shell.execute_reply.started":"2024-08-12T19:35:21.433695Z","shell.execute_reply":"2024-08-12T19:35:21.451482Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\",token=True)\ntokenizer = NllbTokenizerFast.from_pretrained(\"facebook/nllb-200-distilled-600M\",token=True, src_lang = \"eng_Latn\", tgt_lang=\"deu_Latn\",use_fast=\"False\")\n#tokenizer_rev = NllbTokenizerFast.from_pretrained(\"facebook/nllb-200-distilled-600M\",token=True, src_lang = \"deu_Latn\", tgt_lang=\"eng_Latn\",use_fast=\"False\")\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Reading europarl dataset for ENGLISH ====> GERMAN\n# Preprocess data\n\nf = pd.read_csv(\"/kaggle/input/europarl-pronoun-data-it/filtered_europarl_pronoun_dataset_uniform_mult_pro.csv\")\n\n#f = open(\"/kaggle/input/europarl-nozip/europarl-v7.de-en.en\")\n#g = open(\"/kaggle/input/europarl-nozip/europarl-v7.de-en.de\")\n\n\nsrc_sents = f.iloc[:,0].tolist()\ntgt_sents = f.iloc[:,1].tolist()\ntgt_pronouns = f.iloc[:,2].tolist()\n\n#src_sents = f.readlines()\n#tgt_sents = g.readlines()","metadata":{"execution":{"iopub.status.busy":"2024-08-12T19:29:36.294258Z","iopub.execute_input":"2024-08-12T19:29:36.294969Z","iopub.status.idle":"2024-08-12T19:29:48.718576Z","shell.execute_reply.started":"2024-08-12T19:29:36.294937Z","shell.execute_reply":"2024-08-12T19:29:48.717812Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dfae5560bf74e159699d13c08212734"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c56bb75a4224247baef873e736a2692"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9160f4f4984465ab9b4dd0bc157fcf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81abb0cdb53444bf8507e6af19121c29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a4eeb478b4a4e738faa010af8e5f91a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5b71dfaf0aa4a038a14398188fb8b3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ebe25b698a3479a832c361da98d522a"}},"metadata":{}}]},{"cell_type":"code","source":"# Loading transformer\nVAL_SIZE = 84\ntrain_src = src_sents[:-VAL_SIZE]\nval_src = src_sents[-VAL_SIZE:]\ntrain_tgt = tgt_sents[:-VAL_SIZE]\nval_tgt = tgt_sents[-VAL_SIZE:]\nvalid_data = pd.DataFrame({'src_sents':train_src, 'tgt_sents':train_tgt})\nBATCH_SIZE = 4\nvalid_data_hf = Dataset.from_pandas(valid_data)\ndef collator(data):\n    return dict((key, [d[key] for d in data]) for key in data[0])\n\n#Preprocess validation set\nvalidation = pd.DataFrame({'src_sents':val_src, 'tgt_sents':val_tgt})\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T19:32:44.386550Z","iopub.execute_input":"2024-08-12T19:32:44.387312Z","iopub.status.idle":"2024-08-12T19:32:44.482535Z","shell.execute_reply.started":"2024-08-12T19:32:44.387281Z","shell.execute_reply":"2024-08-12T19:32:44.481638Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import re \n\n# Training loop\n# Loading model parameters and initialising trainer\n# model_ref = create_reference_model(model)\ndevice=\"cuda\"\nmodel = model.to(device)\nmodel_ref = model.to(device)\ndataloader = torch.utils.data.DataLoader(valid_data_hf, batch_size=BATCH_SIZE, shuffle=False, collate_fn = collator)\nsteps = 0\nite = 0\nrewards = []\nplot_s_reward = []\nplot_p_reward = []\nplot_fin_reward = []\nplot_fin_alt_reward = []\navg_rew = deque(maxlen=15)\navg_fin_reward_dq =deque(maxlen=15)\navg_fin_alt_reward_dq =deque(maxlen=15)\navg_s_rew_dq = deque(maxlen=15)\navg_p_rew_dq = deque(maxlen=15)\navg_s_rew = []\navg_p_rew = []\navg_rew2 = []\navg_fin_reward = []\navg_fin_alt_reward = []\nplot_pb_reward = []\nplot_m_reward = []\nplot_fin_batch_reward = []\nplot_fin_alt_batch_reward = []\navg_pb_rew_dq = deque(maxlen=15)\navg_fin_batch_reward_dq = deque(maxlen=15)\navg_fin_alt_batch_reward_dq = deque(maxlen=15)\navg_m_rew_dq = deque(maxlen=15)\navg_pb_rew = []\navg_fin_batch_rew = []\navg_m_rew = []\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=4, \n    gradient_accumulation_steps=16,\n    output_dir=\"/kaggle/working/\",\n    learning_rate=6e-6,\n    overwrite_output_dir=True,\n    max_steps=1000,\n    gradient_checkpointing=True,\n    optim=\"adafactor\",\n    report_to=\"none\",\n    save_strategy=\"no\",\n    logging_strategy=\"no\",\n    logging_steps=None\n)\nval_scores = []\nval_pronoun_scores = []\nfin_scores = []\nfin_scores_alt = []\nno_of_positives = []\ntrainer = IterativeSFTTrainer(model=model,args=training_args,tokenizer=tokenizer)\nimport os\nos.environ['WANDB_DISABLED'] = 'true' \ncts = [0,0,0]\nfor sample in dataloader:\n    ite += 1\n    sample_high = []\n    sample_label = []\n    print(\"###############################################\")\n    print(\"Batch no: \",ite)\n    print(\"\\n\")\n    query = tokenizer(sample[\"src_sents\"],return_tensors=\"pt\",padding=True,truncation=True).input_ids.to(device)\n    batch_reward = []\n    batch_fin_reward = []\n    batch_fin_alt_reward = []\n    batch_pronoun_reward = []\n    input_query_list = []\n    input_responses_list = []\n    avg_reward = []\n    #print(\"Source\",sample[\"src_sents\"])\n    is_pro_correct = 0\n    token_lengths = []\n    for j in range(BATCH_SIZE):\n        print(\"Sample no: \",j+1)\n        rew = []\n        unique_translations = []\n        pronoun_reward = []\n        it = 0\n        \n        '''match = re.search(r'</context>(.*)', sample[\"src_sents\"][j])\n        if match:\n            relevant_text = match.group(1).strip()\n        else:\n            raise ValueError(\"The provided text does not contain '</context>'\")\n        \n        token_length = len(tokenizer.tokenize(relevant_text))\n        '''\n        \n        token_length = len(tokenizer.tokenize(sample[\"src_sents\"][j]))\n        #here\n        '''\n        print(\"TARGET SENTENCE: \",sample[\"tgt_sents\"][j])\n        print(\"PRONOUN IN TARGET SENTENCE: \",tgt_pronouns[j+(4*(ite-1))])\n        print(f\"Token length of the source sentence: {token_length}\")\n        print(\"\\n\")\n        '''\n        while len(unique_translations)<10:\n            it = it + 1\n            if it>=25 and len(unique_translations)<4:\n                break\n            if it>=50:\n                break\n            response = model.generate(\n                    tokenizer(sample[\"src_sents\"][j],return_tensors=\"pt\",padding=True,truncation=True).input_ids.to(device),\n                    forced_bos_token_id=tokenizer.lang_code_to_id[\"deu_Latn\"],\n                    do_sample = True,\n                    num_beams= 1,\n                    return_dict_in_generate=True, \n                    output_scores=True\n                    )\n            translation = tokenizer.decode(response.sequences[0], skip_special_tokens=True)\n            pronoun_reward.append(get_pronoun_reward(response,j+(4*(ite-1)),[0,0,0])[0])\n            # print(response.scores)\n            flag = True\n            for i in unique_translations:\n                if translation == i:\n                    #here\n                    pronoun_reward = pronoun_reward[:-1]\n                    flag = False\n            if flag:\n                unique_translations.append(translation)\n        inp_ref = []\n        inp_src = []\n        for i in range(len(unique_translations)):\n            inp_ref.append(sample[\"tgt_sents\"][j])\n            inp_src.append(sample[\"src_sents\"][j])\n        comet_input = []\n        #print(\"Refs\",sample[\"tgt_sents\"][j])\n        for i in range(len(unique_translations)):\n            data = {}\n            data[\"mt\"]=unique_translations[i]\n            data[\"src\"]=sample[\"src_sents\"][j]\n            #here\n            #data[\"pro\"]=pronoun_reward[i]\n            #data[\"ref\"]=sample[\"tgt_sents\"][j]\n            comet_input.append(data)                                                                                                                                                                \n        #print(comet_input)\n        y_high, ind_best= gen_hf(comet_input,token_length,pronoun_reward)\n\n        #Train Reward Plot\n        model_best_response = model.generate(\n                    tokenizer(sample[\"src_sents\"][j],return_tensors=\"pt\",padding=True,truncation=True).input_ids.to(device),\n                    forced_bos_token_id=tokenizer.lang_code_to_id[\"deu_Latn\"],\n                    do_sample = True,\n                    top_k=5,\n                    return_dict_in_generate=True, \n                    output_scores=True\n        )\n        translation = tokenizer.decode(model_best_response.sequences[0],skip_special_tokens=True)\n        #here\n        print(\"\\n Model best response: \",translation)\n        pro_rew,cts = get_pronoun_reward(model_best_response,j+(4*(ite-1)),cts)\n        print(\"Pronoun reward: \",pro_rew)\n        '''\n        if steps%25 == 0:\n            print(f\"For sent {j} in batch: \")\n            print(\"Source sentence with context: \",sample[\"src_sents\"][j])\n            print(\"Source sentence without context: \",relevant_text)\n            print(\"Best candidate translation: \",unique_translations[ind_best])\n            print(\"Best translation for plot: \",translation)\n            print(\"Ref (actual) translation: \",sample[\"tgt_sents\"][j])\n            print(\"Pronoun reward: \",pro_rew)\n        '''\n        comet_input = []\n        data = {}\n        data[\"mt\"] = translation\n        data[\"src\"] = sample[\"src_sents\"][j]\n        #data[\"ref\"] = sample[\"tgt_sents\"][j]\n        comet_input.append(data)\n        highest = calc_comet_score(comet_input)\n        plot_p_reward.append(pro_rew)\n        plot_s_reward.append(highest)\n        avg_p_rew_dq.append(pro_rew)\n        avg_s_rew_dq.append(highest)\n        avg_p_rew.append(np.mean(avg_p_rew_dq))\n        avg_s_rew.append(np.mean(avg_s_rew_dq))\n        batch_pronoun_reward.append(pro_rew)\n        batch_reward.append(highest)\n        sample_high.append(y_high)\n        batch_fin_reward.append((pro_rew/token_length)+highest)\n        batch_fin_alt_reward.append((pro_rew/10)+highest)\n        token_lengths.append(token_length)\n        print(\"\\n\")\n    sample_src = sample['src_sents']\n    mean_r = np.mean(batch_reward)\n    mean_p = np.mean(batch_pronoun_reward)\n    print(\"Batch mean train reward: \",mean_r)\n    print(\"Batch mean pronoun train reward: \",mean_p)\n    plot_m_reward.append(mean_r)\n    plot_pb_reward.append(mean_p)\n    avg_m_rew_dq.append(mean_r)\n    avg_pb_rew_dq.append(mean_p)\n    avg_m_rew.append(np.mean(avg_m_rew_dq))\n    avg_pb_rew.append(np.mean(avg_pb_rew_dq))\n    plot_fin_batch_reward.append(np.mean(batch_fin_reward))\n    plot_fin_alt_batch_reward.append(np.mean(batch_fin_alt_reward))\n    avg_fin_reward_dq.append(np.mean(batch_fin_reward))\n    avg_fin_alt_reward_dq.append(np.mean(batch_fin_alt_reward))\n    avg_fin_reward.append(np.mean(avg_fin_reward_dq))\n    avg_fin_alt_reward.append(np.mean(avg_fin_alt_reward_dq))\n\n    trainer.step(texts=sample_src,texts_labels=sample_high)\n    steps+=1\n    print(\"###############################################\")\n    print(\"\\n\")\n    if steps==1 or steps%100==0:\n        print(\"#########################################################\")\n        print(\"Performing Validation\")\n        print(\"#########################################################\")\n        src_sents = validation[\"src_sents\"].tolist()\n        tgt_sents = validation[\"tgt_sents\"].tolist()\n        translations = []\n        best_responses = []\n        src_wo_context = []\n        print(\"Current no of wrong, no and right pronouns are: \",cts)\n        for i in range(VAL_SIZE):\n            '''\n            match = re.search(r'</context>(.*)', src_sents[i])\n            if match:\n                relevant_text = match.group(1).strip()\n            else:\n                raise ValueError(\"The provided text does not contain '</context>'\")\n            src_wo_context.append(relevant_text)\n            '''\n            src_wo_context.append(src_sents[i])\n            val_query = tokenizer(src_sents[i],return_tensors=\"pt\",padding=True,truncation=True).input_ids.to(device)\n            model_best_response = model.generate(\n                    val_query,\n                    forced_bos_token_id=tokenizer.lang_code_to_id[\"deu_Latn\"],\n                    do_sample = True,\n                    top_k=5,\n                    return_dict_in_generate=True, \n                    output_scores=True\n                    )\n            translations.append(tokenizer.decode(model_best_response.sequences[0], skip_special_tokens=True))\n            best_responses.append(model_best_response)\n        comet_input = []\n        for i in range(len(validation)):\n            data = {}\n            data[\"mt\"] = translations[i]\n            data[\"src\"] = src_wo_context[i]\n            #data[\"ref\"] = tgt_sents[i]\n            comet_input.append(data)\n        val_mean_score = calc_val_comet_score(comet_input)\n        #here\n        \n        #pronoun_scores = calc_pronoun_score(best_responses)\n        #val_mean_pronoun_score = sum(pronoun_scores)/len(pronoun_scores)\n        #val_pronoun_scores.append(val_mean_pronoun_score)\n        #pos = 0\n        #for i in pronoun_scores:\n        #    if i > 0:\n        #        pos += 1\n        #no_of_positives.append(pos)    \n        #print(\"Current validation pronoun scores are: \\n\",val_pronoun_scores)\n        #print(\"No of correct pronouns chosen are: \\n\",no_of_positives)\n        \n        val_scores.append(val_mean_score)\n        print(\"Current validation Comet scores are: \\n\",val_scores)\n        \n\n    if (steps)%25==0:\n        plt.plot(plot_s_reward,'b',label='Comet reward', alpha = 0.2)\n        plt.plot(avg_s_rew,'r',label='Moving average (over last 15 samples)')\n        plt.xlabel(f\"No of samples\")\n        plt.ylabel(\"Samplewise Comet Training Reward\")\n        plt.legend()\n        plt.savefig(f\"/kaggle/working/Sample_comet_reward_{steps}_t5.png\")\n        plt.show()\n        plt.clf()\n        \n        plt.plot(plot_p_reward,'b',label='Pronoun reward', alpha = 0.2)\n        plt.plot(avg_p_rew,'r',label='Moving average (over last 15 samples)')\n        plt.xlabel(f\"No of samples\")\n        plt.ylabel(\"Samplewise Pronoun Training Reward\")\n        plt.legend()\n        plt.savefig(f\"/kaggle/working/Sample_pronoun_reward_{steps}_t5.png\")\n        plt.show()\n        plt.clf()\n        \n        plt.plot(plot_pb_reward,'b',label='Pronoun reward', alpha = 0.2)\n        plt.plot(avg_pb_rew,'r',label='Moving average (over last 15 samples)')\n        plt.xlabel(f\"No of batches\")\n        plt.ylabel(\"Batchwise Pronoun Training Reward\")\n        plt.legend()\n        plt.savefig(f\"/kaggle/working/Batch_pronoun_reward_{steps}_t5.png\")\n        plt.show()\n        plt.clf()\n        \n        plt.plot(plot_m_reward,'b',label='Comet reward',alpha = 0.2)\n        plt.plot(avg_m_rew,'r',label='Moving average (over last 15 samples)')\n        plt.xlabel(f\"No of batches\")\n        plt.ylabel(\"Batchwise Comet Training Reward\")\n        plt.legend()\n        plt.savefig(f\"/kaggle/working/Batch_comet_reward_{steps}_t5.png\")\n        plt.show()\n        plt.clf()\n        \n        '''\n        plt.plot(plot_fin_alt_batch_reward,'b',label='Pronoun reward (over tokens)', alpha = 0.2)\n        plt.plot(avg_fin_alt_reward,'r',label='Moving average (over last 15 samples)')\n        plt.xlabel(f\"No of batches\")\n        plt.ylabel(\"Batchwise final training reward (alt)\")\n        plt.legend()\n        plt.savefig(f\"/kaggle/working/Batch_fin_alt_reward_{steps}_t5.png\")\n        plt.show()\n        plt.clf()\n        \n        plt.plot(plot_fin_batch_reward,'b',label='Pronoun reward (over 10)', alpha = 0.2)\n        plt.plot(avg_fin_reward,'r',label='Moving average (over last 15 samples)')\n        plt.xlabel(f\"No of batches\")\n        plt.ylabel(\"Batchwise final training reward\")\n        plt.legend()\n        plt.savefig(f\"/kaggle/working/Batch_fin_reward_{steps}_t5.png\")\n        plt.show()\n        plt.clf()\n        '''\n\n        print(\"Current validation Comet scores are: \\n\",val_scores)\n    if (steps)==750 or steps==1000:\n        trainer.push_to_hub(f\"satanicmangoes/europarl_pronoun_nllb_{steps}\")\n        break","metadata":{"execution":{"iopub.status.busy":"2024-08-12T19:35:28.824741Z","iopub.execute_input":"2024-08-12T19:35:28.825031Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"},{"name":"stdout","text":"###############################################\nBatch no:  1\n\n\nSample no:  1\n\n Model best response:  Ich glaube auch, daß die Zeit gekommen ist, daß wir uns einer wesentlichen Überprüfung der Listen der GSP und GSP+ Begünstigten widmen.\nPronoun reward:  0\n\n\nSample no:  2\n\n Model best response:  Das ist doch nur die Sache, die wir in der Vergangenheit übernommen haben.\nPronoun reward:  0\n\n\nSample no:  3\n\n Model best response:  Ich danke also der Kommission für ihren Weißbuch und der Frau Rothe für ihren Bericht sowie allen anderen, die daran gearbeitet haben, ihn so bewundernswert und aussagekräftig zu machen.\nPronoun reward:  0\n\n\nSample no:  4\n\n Model best response:  Es ist daher dringend erforderlich, das zweite Binnenmarktpaket und das dritte Energiepaket zu implementieren. \nPronoun reward:  1.0\n\n\nBatch mean train reward:  0.7203928530216217\nBatch mean pronoun train reward:  0.25\n###############################################\n\n\n#########################################################\nPerforming Validation\n#########################################################\nCurrent no of wrong, no and right pronouns are:  [0, 3, 1]\nCurrent validation Comet scores are: \n [0.639169077433291]\n###############################################\nBatch no:  2\n\n\nSample no:  1\n\n Model best response:  Die Kommission und der Rat werden außerdem eine wichtige Aufgabe erledigen - sie wird diesen Rahmen erläutern, umzuarbeiten und festlegen.\nPronoun reward:  -0.8628786206245422\n\n\nSample no:  2\n\n Model best response:  Ich möchte Ihnen sagen, daß ich mich mit den Änderungsanträgen befaßte, und ich bin mir sicher, daß ich das Wort nicht ausgesprochen habe und daß ich nicht über den Vorschlag des Europäischen Parlaments verfügte.\nPronoun reward:  0\n\n\nSample no:  3\n\n Model best response:  Wir werden wohl bald unser Ziel erreichen, das ist die Annahme dieses Gesetzes, aber das bedeutet zwar nicht - und zwar nicht wirklich -, daß die Schwierigkeiten überwunden wurden, es mag auch sein, daß wir noch mit den meisten Schwierigkeiten zu tun haben, denn das ist eine Handlung, deren Umsetzung noch viel Aufmerksamkeit, Kreativität und Energie erfordert. \nPronoun reward:  1.0\n\n\nSample no:  4\n\n Model best response:  Es ist nicht zulässig, daß es in diesem Moment noch keine Überwachungsstelle gibt.\nPronoun reward:  1.0\n\n\nBatch mean train reward:  0.6178808361291885\nBatch mean pronoun train reward:  0.28428034484386444\n###############################################\n\n\n###############################################\nBatch no:  3\n\n\nSample no:  1\n\n Model best response:  Es ist eine Frage, die ich in der Entschließungsanträge des Berichts über die Kommission und die Europäische Kommission über den Ausdruck von Sicherheit und Sicherheit in Europa möchte, wenn ich die Möglichkeit sehe.\nPronoun reward:  1.0\n\n\nSample no:  2\n\n Model best response:  Es war wichtig, daß die Resolution zur Richtlinie über die Verzögerung der Zahlungen angenommen wird, und deshalb habe ich mit diesem Entschließungsantrag stimmen gelöst.\nPronoun reward:  1.0\n\n\nSample no:  3\n\n Model best response:  Das geht nicht mehr um die Verbesserung der Finanzdienstleistungen, sondern um eine Erweiterung der Kontrolle über Brüssel. \nPronoun reward:  -0.9839215278625488\n\n\nSample no:  4\n\n Model best response:  Aber ich glaube, sie geht nicht den richtigen Weg. Da sie die Kommission mit den meisten Ausübungspapieren befasst.\nPronoun reward:  -0.3242984414100647\n\n\nBatch mean train reward:  0.5911729708313942\nBatch mean pronoun train reward:  0.17294500768184662\n###############################################\n\n\n###############################################\nBatch no:  4\n\n\nSample no:  1\n\n Model best response:  Ich möchte Ihnen sagen, daß der Bericht über die Entwicklung von Strukturfonds im Jahre 1990 nicht mehr in die richtigen und günstigen Mittel eingesetzt werden kann.\nPronoun reward:  1.0\n\n\nSample no:  2\n\n Model best response:  Es wird im nächsten Geschäftsjahresjahr wichtig sein, den Versuch, den Unterschied zwischen den Mitgliedstaaten in bezug auf das BIP auf die gleiche Weise auszuräumen, fortzusetzen. \nPronoun reward:  1.0\n\n\nSample no:  3\n\n Model best response:  Ich habe den Vorschlag für eine Richtlinie zur Einführung von Rechts- und Verfahrensweisen zur Änderung dieser Richtlinie zur Aufhebung von\nPronoun reward:  0\n\n\nSample no:  4\n\n Model best response:  Die Kommission ist der Auffassung, dass der Europäische Parlament im Rahmen dieser Entschließungsentscheidungen, die von der Kommission im Rahmen der Entschließung angenommen wurde, eine wichtige Rolle spielen sollte.\nPronoun reward:  0\n\n\nBatch mean train reward:  0.43117958307266235\nBatch mean pronoun train reward:  0.5\n###############################################\n\n\n###############################################\nBatch no:  5\n\n\nSample no:  1\n\n Model best response:  Es ist bedauerlich, daß der Dialog zwischen den Vertretern des Dalai Lama und den RegierungsvertreterInnen der Volksrepublik China, der Hoffnung auf die Entwicklung der Menschenrechnung gab, sehr langsam voranschlägt. \nPronoun reward:  1.0\n\n\nSample no:  2\n\n Model best response:  In dieser Arbeit sind eine Reihe weiteren interessanter Anschauungen enthalten, wie zum Beispiel die Abschaffung der Subventionen für fosside Brennstoffe bis 2010 und der Abschaffung von 7% Biobrennstoffen - ebenfalls - für Autos und Lastwagen \nPronoun reward:  -0.904630720615387\n\n\nSample no:  3\n\n Model best response:  Der Präsident. kann es ein Zufall sein, daß wir in dieser Woche einen Bericht über die Offenheit in der Union besprechen.\nPronoun reward:  1.0\n\n\nSample no:  4\n\n Model best response:  Ich möchte jedoch sagen, daß wir uns in der Tat nicht über den Rat, sondern über den Parlament, die die Kommission und die Kommission in der Lage sind, das Parlament zu unterstützen, die Probleme zu lösen, die wir mit dieser Frage haben.\nPronoun reward:  0\n\n\nBatch mean train reward:  0.5116847455501556\nBatch mean pronoun train reward:  0.27384231984615326\n###############################################\n\n\n###############################################\nBatch no:  6\n\n\nSample no:  1\n\n Model best response:  Wie gesagt wurde, ist es wahr, daß seit Kairo wenig geschehen ist. \nPronoun reward:  1.0\n\n\nSample no:  2\n\n Model best response:  Daher ist es notwendig, die globale Koordination der kohäsionspolitik auf europäischer Ebene grundlegend zu verbessern. \nPronoun reward:  1.0\n\n\nSample no:  3\n\n Model best response:  Wir haben jetzt über ein gesetz, welches es ermöglichen würde, spionieren, die die gesetze schützen. \nPronoun reward:  1.0\n\n\nSample no:  4\n\n Model best response:  Natürlich ist es besser, ein Geschwindigkeitsschiff als ein Rudeschiff zu haben, da man mit dem Geschwindigkeitsschiff schneller gehen kann, aber wenn man nicht weiß wohin man gehen soll, steht man nicht im Vorteil. \nPronoun reward:  1.0\n\n\nBatch mean train reward:  0.7652434185147285\nBatch mean pronoun train reward:  1.0\n###############################################\n\n\n###############################################\nBatch no:  7\n\n\nSample no:  1\n\n Model best response:  Das ist nicht der Fall, wie man sich in einer Höhle um eine Kerze drübt: es geht um eine Zukunft, die sich positiv und ansprechender ansehen kann als die heute. \nPronoun reward:  1.0\n\n\nSample no:  2\n\n Model best response:  Deshalb macht sie eine so große Wirkung, daß so viele früheren Menschenrechtsaktivisten, hier sitzend, so viele von ihm, Václav Havel, so viele von unseren polnischen Kollegen, so fest bei der Unterstützung des Tibets stehe. Dabei ist es offensichtlich, daß Freiheit in Europa und Tibet auf lange Sicht nicht voneinander trennbar sind, und daß Freiheit nicht unteilbar ist.\nPronoun reward:  1.0\n\n\nSample no:  3\n\n Model best response:  Es war früher gesagt, dass die Raketen aus der Serbischen Republik Scuds von Rom aus leicht erreichen können.\nPronoun reward:  1.0\n\n\nSample no:  4\n\n Model best response:  Das Abkommen zwischen der EU und dem Königreich Marokko hat ebenso wie die anderen keine Konfliktlösungsanstrengungen zur Folge gelegt, die den Handel natürlich nicht unzuverlässlicher machen und den Erwartungsgrad der Parteien bei einer gerechten Lösung im Streitfall verringern haben. \nPronoun reward:  -0.993769109249115\n\n\nBatch mean train reward:  0.7089076042175293\nBatch mean pronoun train reward:  0.5015577226877213\n###############################################\n\n\n###############################################\nBatch no:  8\n\n\nSample no:  1\n\n Model best response:  Es ist gut, daß auch das Europäische Parlament sich ernsthaft auf die Umsetzung der Rechtsvorschriften der Mitgliedstaaten einig wird \nPronoun reward:  1.0\n\n\nSample no:  2\n\n Model best response:  Die Kommission hat jedoch nicht die Absicht, den rechten der Bürger, die Rechtsvorschriften des Parlaments zu beeinträchtigen, so dass sie die Rechtsgrundlage der Kommission nicht berücksichtigen.\nPronoun reward:  -0.30416494607925415\n\n\nSample no:  3\n\n Model best response:  Ich denke auch gar nicht, daß die internationalen Gemeinschaften den Rat von der ganzen Erde mit Kriegen ablösen. \nPronoun reward:  0\n\n\nSample no:  4\n\n Model best response:  Darüber hinaus ist es wichtig, dass der einzelne Mitgliedstaat in der Lage sein kann, sich zu entscheiden, welche Form des Biokraftstoffs der richtigste ist und auf der Grundlage der grundlegenden Bedingungen vorhanden ist. \nPronoun reward:  1.0\n\n\nBatch mean train reward:  0.5757258012890816\nBatch mean pronoun train reward:  0.42395876348018646\n###############################################\n\n\n###############################################\nBatch no:  9\n\n\nSample no:  1\n\n Model best response:  Ich hoffe, daß es heute in den europäischen städten zu einem großer Aufreg ist, daß wir nicht länger diese \"klapperne braune Luft\" atmen müssen.\nPronoun reward:  1.0\n\n\nSample no:  2\n\n Model best response:  Ich bin der Meinung, daß die Mitgliedstaaten, die sich in der Praxis befinden, sich in diesem Sinne nicht auf das Ziel konzentrieren.\nPronoun reward:  0\n\n\nSample no:  3\n\n Model best response:  Und wenn es einen Bauern gibt, der mitten in einem Dorf eine kleine Farme hat, ist es nicht immer einfach, die entsprechenden Flächen für Schleppen zu schaffen.\nPronoun reward:  1.0\n\n\nSample no:  4\n\n Model best response:  Es ist entscheidend, in eine radikale soziale Veränderung, die sich auf die nachhaltigkeit dieser Städte, den dezentralisierten Energetikationsproduktionen und die Wettbewerbsfähigkeit der Industrie stützt, zu investieren. \nPronoun reward:  1.0\n\n\nBatch mean train reward:  0.646475113928318\nBatch mean pronoun train reward:  0.75\n###############################################\n\n\n###############################################\nBatch no:  10\n\n\nSample no:  1\n\n Model best response:  Es wäre ideal, wenn eine Überprüfung über eine verdeckte Rückwärtsentwicklung hinausging und eine reine, umweltgerechtigtere Modulation erreicht, die sich nach einer Reihe von Kriterien, besonders in der Frage der Beschäftigung, ergibt. \nPronoun reward:  1.0\n\n\nSample no:  2\n\n Model best response:  Es wäre nicht ironisch - wäre es nicht ironisch - denn im Falle des Sturzes des \"Sicher Vorhangs\" und der Rückgang der drohenden Absterblick zwischen den Supermächten ist es nicht ironischer, wenn in einigen der unstabilen Orte der Welt ein neuer Abrüstungen-Rennen entstehe.\nPronoun reward:  1.0\n\n\nSample no:  3\n\n Model best response:  Das Budget ist für arbeitslose, die derzeit nach einem neuen Job suchen. \nPronoun reward:  0\n\n\nSample no:  4\n\n Model best response:  Es ist von wesentlicher Bedeutung, die Chancen dieser Fonds zur Schaffung günstiger Betriebsbedingungen für KMU zu nutzen, die Finanzmöglichkeiten zu verbessern und die Beteiligung an verschiedenen innovativen Projekten und der Kooperation auf lokaler und regionaler Ebene zu fördern.\nPronoun reward:  -0.8724695444107056\n","output_type":"stream"}]}]}