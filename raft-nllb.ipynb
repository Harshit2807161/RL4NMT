{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7792886,"sourceType":"datasetVersion","datasetId":4561890}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install trl\n%pip install -U datasets\n!pip install unbabel-comet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyarrow==11.0.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env TOKENIZERS_PARALLELISM=false","metadata":{"execution":{"iopub.status.busy":"2024-03-24T17:37:37.085928Z","iopub.execute_input":"2024-03-24T17:37:37.086638Z","iopub.status.idle":"2024-03-24T17:37:37.098351Z","shell.execute_reply.started":"2024-03-24T17:37:37.086607Z","shell.execute_reply":"2024-03-24T17:37:37.097413Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"env: TOKENIZERS_PARALLELISM=false\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importing dependencies\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom transformers import NllbTokenizerFast,AutoModelForSeq2SeqLM, TrainingArguments\nfrom trl import PPOTrainer, PPOConfig, create_reference_model\nfrom datasets import Dataset\nimport warnings\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings(\"ignore\")\nfrom collections import deque\nfrom datasets import load_dataset\nfrom trl import IterativeSFTTrainer\nfrom comet import download_model, load_from_checkpoint\nimport statistics\nimport logging\nloggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\nfor logger in loggers:\n    logger.setLevel(logging.WARNING)\n\n# Setting device to GPU\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n#Loading reward generation model\nmodel_path = download_model(\"Unbabel/wmt22-cometkiwi-da\")\nreward_gen_model = load_from_checkpoint(model_path)\n\n# Reward functions\ndef gen_hf(data):\n    output = reward_gen_model.predict(data,batch_size = len(data), gpus = 1,progress_bar=False)\n    rewards = output.scores\n    high_ind = np.argmax(rewards)\n    y_high = data[high_ind][\"mt\"]\n    return y_high\n\ndef calc_comet_score(data):\n    output = reward_gen_model.predict(data,batch_size = len(data), gpus = 1,progress_bar=False)\n    rewards = output.scores\n    print(rewards)\n    return rewards[0]\n\ndef calc_val_comet_score(data):\n    output = reward_gen_model.predict(data,batch_size = len(data), gpus = 1,progress_bar=False)\n    rewards = output.scores\n    return sum(rewards)/len(rewards)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T17:37:40.178483Z","iopub.execute_input":"2024-03-24T17:37:40.178866Z","iopub.status.idle":"2024-03-24T17:38:25.190570Z","shell.execute_reply.started":"2024-03-24T17:37:40.178838Z","shell.execute_reply":"2024-03-24T17:38:25.189732Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-03-24 17:37:49.539575: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-24 17:37:49.539671: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-24 17:37:49.679691: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f696ef6a0fd412d8b9d8c1cf4ad2046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.ckpt:   0%|          | 0.00/2.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"152f793d59ae406e9c0cabe0f35a7c9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b6ac652021b4d1daf4794e78a39573e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"LICENSE:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"add2567479b64098ae7d2611f5b8d59f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c277850010a444d18e837e4e0bcbb61a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"hparams.yaml:   0%|          | 0.00/710 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d0c1b7846f940b0874d138f7e772ef6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"939d6da4c4924ff491e68ee9895c3d98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70fad1e45cf14b2c8f708fbce4c8dc0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/513 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79f52986e4c1429abcf46f14664853c9"}},"metadata":{}}]},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\",token=True)\ntokenizer = NllbTokenizerFast.from_pretrained(\"facebook/nllb-200-distilled-600M\",token=True, src_lang = \"deu_Latn\", tgt_lang=\"eng_Latn\",use_fast=\"False\")\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Reading europal dataset for GERMAN ====> ENGLISH\n# Preprocess data\nf = open(\"/kaggle/input/europarl-nozip/europarl-v7.de-en.de\")\ng = open(\"/kaggle/input/europarl-nozip/europarl-v7.de-en.en\")\nsrc_sents = f.readlines()\ntgt_sents = g.readlines()","metadata":{"execution":{"iopub.status.busy":"2024-03-24T17:38:30.003134Z","iopub.execute_input":"2024-03-24T17:38:30.003802Z","iopub.status.idle":"2024-03-24T17:38:53.060139Z","shell.execute_reply.started":"2024-03-24T17:38:30.003767Z","shell.execute_reply":"2024-03-24T17:38:53.059328Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf91f7d758814d43b601e463f27a4b49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca8cbe3ab12a4a4a83595ca4d6125375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9427631e4f33470298f271c48f1e950f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91880ff9ab684d799ad6e120dfa39bc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b63e4d9ac7cd439782943425bafe28ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51565deb2b8043498d743a1dc3873793"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff1ce7e25964d02a128e17f6cb39eeb"}},"metadata":{}}]},{"cell_type":"code","source":"# Loading transformer\nVAL_SIZE = 84\ntrain_src = src_sents[:-VAL_SIZE]\nval_src = src_sents[-VAL_SIZE:]\ntrain_tgt = tgt_sents[:-VAL_SIZE]\nval_tgt = tgt_sents[-VAL_SIZE:]\nvalid_data = pd.DataFrame({'src_sents':train_src, 'tgt_sents':train_tgt})\nBATCH_SIZE = 4\nvalid_data_hf = Dataset.from_pandas(valid_data)\ndef collator(data):\n    return dict((key, [d[key] for d in data]) for key in data[0])\n\n#Preprocess validation set\nvalidation = pd.DataFrame({'src_sents':val_src, 'tgt_sents':val_tgt})\n","metadata":{"execution":{"iopub.status.busy":"2024-03-24T17:38:55.929790Z","iopub.execute_input":"2024-03-24T17:38:55.930597Z","iopub.status.idle":"2024-03-24T17:39:02.122695Z","shell.execute_reply.started":"2024-03-24T17:38:55.930558Z","shell.execute_reply":"2024-03-24T17:39:02.121984Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Training loop\n# Loading model parameters and initialising trainer\n# model_ref = create_reference_model(model)\ndevice=\"cuda\"\nmodel = model.to(device)\nmodel_ref = model.to(device)\ndataloader = torch.utils.data.DataLoader(valid_data_hf, batch_size=BATCH_SIZE, shuffle=True, collate_fn = collator)\nsteps = 0\nite = 0\nrewards = []\nplot_s_reward = []\navg_rew = deque(maxlen=15)\navg_s_rew_dq = deque(maxlen=15)\navg_s_rew = []\navg_rew2 = []\nplot_m_reward = []\navg_m_rew_dq = deque(maxlen=15)\navg_m_rew = []\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=4, \n    gradient_accumulation_steps=16,\n    output_dir=\"/kaggle/working/\",\n    learning_rate=6e-6,\n    overwrite_output_dir=True,\n    max_steps=1000,\n    gradient_checkpointing=True,\n    optim=\"adafactor\",\n    report_to=\"none\"\n)\nval_scores = []\ntrainer = IterativeSFTTrainer(model=model,args=training_args,tokenizer=tokenizer)\nimport os\nos.environ['WANDB_DISABLED'] = 'true' ","metadata":{"execution":{"iopub.status.busy":"2024-03-24T17:40:56.049787Z","iopub.execute_input":"2024-03-24T17:40:56.050610Z","iopub.status.idle":"2024-03-24T17:40:56.105749Z","shell.execute_reply.started":"2024-03-24T17:40:56.050573Z","shell.execute_reply":"2024-03-24T17:40:56.105024Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for sample in dataloader:\n    ite += 1\n    sample_high = []\n    sample_label = []\n    print(\"###############################################\")\n    print(\"Batch no: \",ite)\n    print(\"\\n\")\n    query = tokenizer(sample[\"src_sents\"],return_tensors=\"pt\",padding=True,truncation=True).input_ids.to(device)\n    batch_reward = []\n    input_query_list = []\n    input_responses_list = []\n    avg_reward = []\n    #print(\"Source\",sample[\"src_sents\"])\n    for j in range(BATCH_SIZE):\n        print(\"Sample no: \",j+1)\n        rew = []\n        unique_translations = []                                                                                                                                                                    \n        it = 0\n        while len(unique_translations)<10:\n            it = it + 1\n            if it>=25 and len(unique_translations)<4:\n                break\n            if it>=50:\n                break\n            response = model.generate(\n                    tokenizer(sample[\"src_sents\"][j],return_tensors=\"pt\",padding=True,truncation=True).input_ids.to(device),\n                    forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"],\n                    do_sample = True,\n                    num_beams= 1\n                    )\n            translations = tokenizer.decode(response[0], skip_special_tokens=True)\n            # print(response.scores)\n            rew.append(translations)\n            array_of_strings = np.array(rew)\n            #print(rew)\n            a = len(unique_translations)\n            unique_translations = np.unique(array_of_strings)\n            if a!=len(unique_translations):\n                input_responses_list.append(response[0])\n                input_query_list.append(query[j])                                                                                                                                                   \n        inp_ref = []\n        inp_src = []\n        for i in range(len(unique_translations)):\n            inp_ref.append(sample[\"tgt_sents\"][j])\n            inp_src.append(sample[\"src_sents\"][j])\n        comet_input = []\n        #print(\"Refs\",sample[\"tgt_sents\"][j])\n        #print(\"Translations\",unique_translations)\n        for i in range(len(unique_translations)):\n            data = {}\n            data[\"mt\"]=unique_translations[i]\n            data[\"src\"]=inp_src[i]\n            comet_input.append(data)                                                                                                                                                                \n        #print(comet_input)\n        y_high= gen_hf(comet_input)\n\n        #Train Reward Plot\n        model_best_response = model.generate(\n                    tokenizer(sample[\"src_sents\"][j],return_tensors=\"pt\",padding=True,truncation=True).input_ids.to(device),\n                    forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"],\n                    do_sample = True,\n                    top_k=5\n        )\n        translation = tokenizer.decode(model_best_response[0],skip_special_tokens=True)\n        comet_input = []\n        data = {}\n        data[\"mt\"] = translation\n        data[\"src\"] = sample[\"src_sents\"][j]\n        comet_input.append(data)\n        highest = calc_comet_score(comet_input)\n        plot_s_reward.append(highest)\n        avg_s_rew_dq.append(highest)\n        avg_s_rew.append(np.mean(avg_s_rew_dq))\n        batch_reward.append(highest)\n        sample_high.append(y_high)                                                                                                                                                                                        \n    sample_src = sample['src_sents']\n    mean_r = np.mean(batch_reward)\n    print(\"Batch mean train reward: \",mean_r)\n    plot_m_reward.append(mean_r)\n    avg_m_rew_dq.append(mean_r)\n    avg_m_rew.append(np.mean(avg_m_rew_dq))\n    trainer.step(texts=sample_src,texts_labels=sample_high)\n    steps+=1\n    print(\"###############################################\")\n    print(\"\\n\")\n    if steps==1 or steps%100==0:\n        print(\"#########################################################\")\n        print(\"Performing Validation\")\n        print(\"#########################################################\")\n        src_sents = validation[\"src_sents\"].tolist()\n        val_query = tokenizer(src_sents,return_tensors=\"pt\",padding=True,truncation=True).input_ids.to(device)\n        model_best_response = model.generate(\n                    val_query,\n                    forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"],\n                    do_sample = True,\n                    top_k=5\n                    )\n        translations = tokenizer.batch_decode(model_best_response,skip_special_tokens=True)                                                                                                         \n        comet_input = []\n        for i in range(len(validation)):\n            data = {}\n            data[\"mt\"] = translations[i]\n            data[\"src\"] = src_sents[i]\n            comet_input.append(data)\n        val_mean_score = calc_val_comet_score(comet_input)\n        val_scores.append(val_mean_score)\n        print(\"Current validation Comet scores are: \\n\",val_scores)\n\n    if (steps)%25==0:\n        plt.plot(plot_s_reward,'b',label='reward')\n        plt.plot(avg_s_rew,'r',label='Moving average (over last 15 samples)')\n        plt.xlabel(f\"No of samples\")\n        plt.ylabel(\"Samplewise Training Reward\")\n        plt.legend()\n        plt.savefig(f\"/kaggle/working/reward_plots/Sample_rew_{steps}_t5.png\")\n        plt.show()\n        plt.clf()\n\n        plt.plot(plot_m_reward)\n        plt.plot(avg_m_rew)\n        plt.xlabel(f\"No of batches\")\n        plt.ylabel(\"Batchwise Training Reward\")\n        plt.legend()\n        plt.savefig(f\"/kaggle/working/reward_plots/Batch_rew_{steps}_t5.png\")\n        plt.show()\n        plt.clf()\n        print(\"Current validation Comet scores are: \\n\",val_scores)\n    if (steps)==1000:\n        trainer.push_to_hub(f\"satanicmangoes/RAFT+_nllb_{steps}\")\n        break","metadata":{"execution":{"iopub.status.busy":"2024-03-24T17:41:27.686606Z","iopub.execute_input":"2024-03-24T17:41:27.686872Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"###############################################\nBatch no:  1\n\n\nSample no:  1\n[0.8242360353469849]\nSample no:  2\n[0.37044331431388855]\nSample no:  3\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Specify the directory path\ndirectory_path = '/kaggle/working/reward_plots'\n\n# Create the directory\nos.makedirs(directory_path, exist_ok=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-24T17:40:36.680611Z","iopub.execute_input":"2024-03-24T17:40:36.681209Z","iopub.status.idle":"2024-03-24T17:40:36.685662Z","shell.execute_reply.started":"2024-03-24T17:40:36.681173Z","shell.execute_reply":"2024-03-24T17:40:36.684641Z"},"trusted":true},"execution_count":9,"outputs":[]}]}